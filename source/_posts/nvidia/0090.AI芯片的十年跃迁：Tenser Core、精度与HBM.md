---
title: 9. AI芯片的十年跃迁：Tenser Core、精度与HBM
date: 2025-10-21 16:50:46
categories:
  - NVIDIA
top_order: 90
---

2017 年，NVIDIA V100 在小集群上训练 ResNet-50 需要约 8 小时。

2023 年，H100 在大规模集群基准中完成类似任务只需 10 分钟，性能提升近 50 倍。

但硬件资源只增长了 3.8 倍：

- 晶体管从 211 亿到 800 亿
- 功耗从 300W 到 700W
- 制程从 16nm 到 4nm。

**额外的 13 倍从哪来？**

<!--more-->

---

## 第一章：深度学习暴露的新瓶颈

### 2012 年的意外发现

AlexNet 在 ImageNet 比赛中用 GPU 训练，比 CPU 快 50 倍。原因很简单：卷积神经网络的核心操作是矩阵乘法，而矩阵乘法天然适合并行。一个 3×3 卷积核滑过图像时，每个位置的计算完全独立，GPU 的数千个核心可以同时工作。

但 NVIDIA 的工程师发现了一个问题：GPU 虽然快，实际利用率只有 30-40%。Tesla P100 理论峰值性能 10.6 TFLOPS（FP32），但训练 ResNet-50 时实测只达到 3.5 TFLOPS。剩下的 70%算力去哪了？

### 深度学习不是图形渲染

GPU 最初为图形渲染设计，但深度学习的计算模式完全不同：

| 特征         | 图形渲染             | 深度学习                 | 差异             |
| ------------ | -------------------- | ------------------------ | ---------------- |
| **主要操作** | 复杂着色器计算       | 90%是矩阵乘法            | 单一操作占主导   |
| **数据复用** | 低（每像素独立）     | 极高（权重被重用数千次） | 100 倍差异       |
| **精度需求** | FP32（颜色必须精确） | FP16 甚至 FP8 可接受     | 可降低 2-4 倍    |
| **内存访问** | 随机纹理读取         | 连续大块矩阵             | 访问模式完全不同 |

关键洞察：CUDA Core 是"全科医生"，可以做任何计算。但 AI 只需要"矩阵乘法专科医生"。

### CUDA Core 的三大瓶颈

让我们看一个具体例子。计算两个 512×512 的矩阵相乘（C = A × B），这是神经网络中最常见的操作。

**CUDA Core 的工作方式**：

```
计算C[0,0]一个元素：
1. 从内存读取A的第0行（512个数）
2. 从内存读取B的第0列（512个数）
3. 执行512次乘法和511次加法
4. 写回C[0,0]

问题：
• 计算C的全部262,144个元素（512×512）
• 每个元素需要读512个数
• A和B的每个元素被读取512次
• 总内存读取：262,144 × 512 × 2 = 2.68亿次
```

三大瓶颈浮现：

**瓶颈 1：内存带宽浪费**。权重矩阵 B 被重复读取 512 次，但 GPU 的内存带宽只有 900 GB/s（V100）。如果矩阵是 FP32（4 字节），单次加载 A 和 B 就需要 2 MB，重复 512 次就是 1 GB。即使带宽全开，也需要 1.1 毫秒只为了搬运数据。

**瓶颈 2：寄存器压力**。每个 CUDA Core 需要保存部分和（partial sum），但 SM 的寄存器堆有限（256 KB）。当矩阵变大时，中间结果必须溢出到 Shared Memory 甚至全局内存，进一步降低性能。

**瓶颈 3：通用性代价**。CUDA Core 包含完整的 FP32 流水线（乘法器、加法器、舍入逻辑），约 3.5 万个晶体管。但矩阵乘法只需要重复"乘法+加法"，不需要除法、平方根、三角函数这些复杂功能。通用性让每个核心变得"笨重"。

**实测数据**（Tesla P100，矩阵乘法 benchmarks）：

- 理论峰值：10.6 TFLOPS
- 小矩阵（128×128）：2.1 TFLOPS（20%效率）
- 中矩阵（512×512）：3.5 TFLOPS（33%效率）
- 大矩阵（4096×4096）：6.8 TFLOPS（64%效率）

只有在非常大的矩阵时，数据复用才能摊薄内存开销。但许多神经网络层的矩阵并不大。

**这一章的答案**：通用并行不够，AI 需要专用电路。就像造飞机不能只改进汽车引擎，需要全新的喷气发动机。

---

## 第二章：Tensor Core——专用矩阵引擎的诞生

### 重新设计：如果只做矩阵乘法

NVIDIA 在 2017 年做了一个决定：用 15%的芯片面积，造一个"只会做矩阵乘法"的专用电路。这就是 Tensor Core。

让我们从一个小矩阵开始理解。计算一个 4×4 的矩阵乘法：D = A × B + C（这个"+C"很重要，后面会解释）。

**CUDA Core 的方式**：

```
把16个CUDA Core组织成一个工作组：
• 第1个核心算D[0,0] = A第0行 × B第0列 + C[0,0]（需要4次乘加）
• 第2个核心算D[0,1] = A第0行 × B第1列 + C[0,1]
• ...
• 第16个核心算D[3,3]

总操作数：16个元素 × 4次乘加 = 64次FMA

内存访问：
• 读取A：4×4 = 16个数
• 读取B：4×4 = 16个数
• 读取C：4×4 = 16个数
• 写入D：4×4 = 16个数
• 总计：64次内存访问

效率：64次计算 / 64次访问 = 1 FLOP/Byte（很低）
```

**Tensor Core 的方式：Systolic Array（脉动阵列）**

想象一个 4×4 的计算器网格。每个格子是一个 MAC 单元（Multiply-Accumulate，乘加器）：

```
    B数据从上往下流
    ↓  ↓  ↓  ↓
  →[●][●][●][●]← A数据从左往右流
  →[●][●][●][●]
  →[●][●][●][●]
  →[●][●][●][●]
    ↓  ↓  ↓  ↓
    结果从下流出
```

工作流程（生动版）：

**周期 0**：A 的第 0 行开始从左边"注入"网格，B 的第 0 列从上面"注入"。

- 左上角格子[0,0]收到：A[0,0]和 B[0,0]
- 计算：partial_sum = A[0,0] × B[0,0]
- 同时，把 A[0,0]传递给右边的格子，把 B[0,0]传递给下面的格子

**周期 1**：数据在网格中"流动"

- 格子[0,0]收到：A[0,1]和 B[1,0]
- 更新：partial_sum += A[0,1] × B[1,0]
- 格子[0,1]收到：A[0,0]（来自左边）和 B[0,1]（来自上面）
- 开始计算 D[0,1]的第一项

**周期 2-3**：数据继续流动，每个格子累加自己的 partial_sum

**周期 4**：所有 16 个结果同时产生！

关键魔法：**每个数据在流过网格时，被多个格子"顺便"使用，不需要重复从内存读取**。

内存访问：

- 读取 A：4×4 = 16 个数（只一次）
- 读取 B：4×4 = 16 个数（只一次）
- 读取 C：4×4 = 16 个数（只一次）
- 写入 D：4×4 = 16 个数（只一次）
- **总计：64 次内存访问（和 CUDA Core 相同）**

但计算是并行的：16 个 MAC 单元同时工作，每个做 4 次乘加，总共 64 次 FMA 在 4 个周期内完成。

等等，内存访问数量没变？为什么更快？

### 真正的优势：数据流 vs 控制流

差异在于**数据流动的方式**：

**CUDA Core（控制流）**：

```
对每个结果D[i,j]：
  1. 调度器选择一个空闲的CUDA Core
  2. 从寄存器堆读取A[i,0], B[0,j], partial_sum
  3. 执行MAC运算
  4. 写回partial_sum
  5. 重复步骤2-4 共4次
  6. 将final result写回C[i,j]

开销：
• 每次操作都需要调度器介入（指令发射）
• 从寄存器堆读写：通过Crossbar（交叉开关）
• 64个结果 × 4次迭代 = 256次调度 + 768次寄存器访问
```

**Tensor Core（数据流）**：

```
一次性设置：
  1. 把A和B加载到Tensor Core内部的输入缓冲区
  2. 启动脉动阵列
  3. 数据自动流动，无需调度器干预
  4. 4个周期后，结果直接出现在输出缓冲区

开销：
• 只需1次调度（启动整个矩阵运算）
• 数据在Tensor Core内部流动，不经过寄存器堆
• 总共只有2次寄存器访问（输入和输出）
```

效率对比（实测 V100，4×4 矩阵块重复 1000 次）：

| 指标              | CUDA Core          | Tensor Core  | 倍数 |
| ----------------- | ------------------ | ------------ | ---- |
| 延迟（单次 4×4）  | 16 cycles          | 4 cycles     | 4×   |
| 吞吐量（1000 次） | 16,000 cycles      | 4,000 cycles | 4×   |
| 调度开销          | 256,000 次指令发射 | 1,000 次     | 256× |
| 寄存器访问        | 768,000 次         | 2,000 次     | 384× |

**关键洞察**：Tensor Core 把"256 次调度+768 次寄存器访问"变成了"硬件自动流水线"。这就像从"人工调度的流水线"升级到"全自动传送带"。

### 扩展到真实规模

V100 的 Tensor Core 不是 4×4，而是处理**16×16 矩阵**（FP16 精度）。每个 SM 有 4 个 Tensor Core。

计算规模：

- 单个 Tensor Core：16×16×16 = 4096 次 FMA 等效，**单周期完成**
- 4 个 Tensor Core：16,384 FMA/周期
- 80 个 SM × 4 = 320 个 Tensor Core：5,242,880 FMA/周期
- 频率 1.53 GHz：**125 TFLOPS**（FP16）

对比 CUDA Core 的 15.7 TFLOPS（FP16），Tensor Core 快了**8 倍**。

### 代价与局限

Tensor Core 不是魔法，有明确的代价：

**晶体管成本**：

- 单个 CUDA Core：3.5 万晶体管
- 单个 Tensor Core：约 1.5 亿晶体管（包含 256 个 MAC 单元、输入缓冲、控制逻辑）
- **成本比**：1 Tensor Core = 4285 个 CUDA Core 的晶体管

但记住，1 个 Tensor Core 的性能（16×16 矩阵/周期）等于 256 个 CUDA Core。所以实际上用了 17 倍的晶体管，换来了 256 倍的峰值性能——**效率提升 15 倍**。

**能效比**：

- CUDA Core：约 2 GFLOPS/W
- Tensor Core（V100）：约 40 GFLOPS/W
- **能效提升 20 倍**

**限制**：

1. 只支持固定大小：16×16（FP16）或 8×8（FP32）
2. 需要编译器支持：自动识别代码中的矩阵乘法
3. 小矩阵不划算：如果只算一个 4×4 矩阵，启动开销反而更大

**真实应用数据**（NVIDIA 官方 benchmark，V100）：

- ResNet-50 训练：CUDA Core 360 images/s → Tensor Core 1230 images/s（**3.4 倍**）
- BERT-Large 训练：吞吐量提升**3.7 倍**
- 为什么不是 8 倍？因为神经网络不是 100%矩阵乘法，还有激活函数、BatchNorm 等操作仍需 CUDA Core

### 为什么不全用 Tensor Core？

既然效率高 20 倍，为什么不把整个 GPU 都做成 Tensor Core？

答案在于**神经网络的计算分布**：

```
典型的Transformer模型（如GPT）计算量分布：
├─ 矩阵乘法（QKV投影、FFN）：90%
├─ Softmax：5%
├─ LayerNorm：3%
└─ 激活函数（GeLU）：2%

Tensor Core只能做第一项（90%）
剩下10%必须用CUDA Core或Special Function Units
```

所以 V100 的设计：

- Tensor Core 占芯片面积：15%
- CUDA Core 占芯片面积：60%
- 其他（缓存、调度器等）：25%

这个比例在后续架构中基本保持不变（A100 和 H100 也是 15% Tensor Core），因为它已经覆盖了最耗时的操作。

**本章结论**：Tensor Core 用专用电路把"控制流"变成"数据流"，用 17 倍晶体管换来 15 倍效率和 20 倍能效比。但它只是拼图的一块，要达到 50 倍提升，还需要两场革命。

---

## 第三章：精度革命——从 FP32 到 FP8 的跃迁

### 一个反直觉的发现

2017 年，百度的研究人员做了一个实验：用 FP16（16 位浮点数）训练 ResNet-50，最终精度与 FP32 相比只下降了 0.1%。这个结果令人震惊——神经网络为什么能容忍一半的精度损失？

更激进的发现来自 2020 年：NVIDIA 证明大部分推理任务可以用 INT8（8 位整数），某些场景甚至可以用 INT4。这意味着精度可以降低**4-8 倍**，而模型仍然有效。

问题是：为什么数值计算可以这么"粗糙"？

### 神经网络的"模糊数学"

传统科学计算（如天气预报、核物理模拟）需要极高精度。一个小误差会在迭代中放大，最终导致结果完全错误。但神经网络不同。

**直白版类比**：

```
传统计算 = 工程制图
• 每条线必须精确到0.01毫米
• 误差累积会导致零件无法组装
• 需要FP64甚至更高精度

神经网络 = 素描画
• 用粗笔勾勒轮廓也能识别
• 关键是"整体趋势"而非"精确数值"
• FP16甚至FP8就够了
```

数学原因：

**原因 1：梯度下降本身就有噪声**。训练神经网络时，每次更新权重使用的是 mini-batch 的梯度，这本身就是全量梯度的"粗糙估计"。如果 batch size 是 256，梯度的方差约为 √256 = 16 倍。相比之下，FP32→FP16 的精度损失（2^11 → 2^10，约 2 倍）根本不重要。

**原因 2：过拟合的保护**。低精度相当于给权重加了"噪声"，这反而能防止模型记住训练数据的细节。许多实验发现 FP16 训练的泛化性能甚至比 FP32 更好。

**原因 3：激活函数的"压缩"**。神经网络中的 ReLU、Sigmoid、Tanh 都会把数值压缩到有限范围。即使前面的计算有误差，激活函数会"校正"它。

让我们看看不同精度的实际表示能力：

| 精度 | 格式                  | 表示范围  | 最小间隔  | 能区分的数量 |
| ---- | --------------------- | --------- | --------- | ------------ |
| FP32 | 1 符号+8 指数+23 尾数 | ±3.4×10³⁸ | ~1.2×10⁻⁷ | 40 亿        |
| FP16 | 1 符号+5 指数+10 尾数 | ±65,504   | ~0.001    | 6.5 万       |
| BF16 | 1 符号+8 指数+7 尾数  | ±3.4×10³⁸ | ~0.01     | 6.5 万       |
| FP8  | 1 符号+4 指数+3 尾数  | ±240      | ~1        | 256          |

**关键观察**：神经网络的权重大多集中在[-1, 1]范围内。在这个范围，FP16 可以区分约 1000 个不同的值，FP8 可以区分约 16 个值。对于"学习趋势"来说，16 个档位已经够用。

### 多精度演进的性能阶梯

每一代 GPU 都在推动精度降低：

**2017 年 - Volta 架构（V100）**：

- 首次引入 Tensor Core
- 支持 FP16：相比 FP32 性能翻倍（2×）
- 原因：单个 MAC 单元可以在相同时钟周期内处理 2 个 FP16 或 1 个 FP32

**2020 年 - Ampere 架构（A100）**：

- 引入 TF32（TensorFloat32）：保持 FP32 的范围，但精度降低到 19 位
- 支持 INT8：推理性能再快 4 倍（相比 FP16）
- 结构化稀疏：进一步 2 倍加速（后面会讲）

**2022 年 - Hopper 架构（H100）**：

- FP8 支持：E4M3（训练）和 E5M2（推理）
- 相比 TF32 性能提升 8 倍
- Transformer Engine：动态精度切换

性能提升表：

| 架构   | 年份 | 最低精度 | Tensor 性能 | 相对 V100 FP32 |
| ------ | ---- | -------- | ----------- | -------------- |
| Volta  | 2017 | FP16     | 125 TFLOPS  | 16×            |
| Ampere | 2020 | INT8     | 624 TOPS    | 80×            |
| Hopper | 2022 | FP8      | 1513 TFLOPS | 190×           |

等等，190 倍性能提升，但我们说的是 50 倍实际训练速度。差距在哪？

### 混合精度的现实

理论峰值是"只做矩阵乘法"的速度。实际训练需要：

**混合精度训练的标准流程**：

```
前向传播：
  1. 权重存储为FP8
  2. 激活值计算用FP16（更高精度）
  3. Tensor Core用FP8×FP8 → FP16累加

反向传播：
  1. 梯度计算用FP16
  2. 权重更新：先转为FP32，更新后转回FP8

为什么需要混合？
• FP8范围太小（±240），直接更新权重容易溢出
• 累加器必须高精度，否则误差累积
```

这意味着不是所有操作都能用 FP8。实测中：

- 矩阵乘法：90%用 FP8
- 其他操作：用 FP16 或 FP32
- 权重更新：必须 FP32

**真实性能数据**（H100 vs A100，GPT-3 175B 训练）：

```
理论加速比：
FP8 TFLOPS / TF32 TFLOPS = 1513 / 312 = 4.85×

实际加速比：
GPT-3训练时间：A100 34天 → H100 11天 = 3.1×

差距原因：
• 通信开销：多卡训练的AllReduce不能完全用FP8
• 非矩阵操作：Softmax、LayerNorm仍需FP16
• 精度转换：FP8↔FP16有开销（约5%时间）
• 内存带宽：部分时候受限于HBM而非计算
```

### Transformer Engine 的智能切换

H100 引入了硬件级别的"动态精度"：

**工作原理**（简化版）：

```
每个Tensor Core包含精度检测电路：

  if (input_tensor的最大值 > 240):  # FP8会溢出
      使用FP16模式
  else:
      使用FP8模式

  累加器始终用FP32（避免误差累积）
```

这个判断在**硬件层面**完成，无需 CPU 干预，开销只有 1 个时钟周期。

效果：

- Attention 的 Q×K 矩阵：通常用 FP8（值域小）
- Softmax 之后的概率：用 FP16（需要更高精度）
- FFN 的第一层：用 FP8（值域大但容忍度高）

**实测结果**（H100，LLaMA 65B 训练）：

- 纯 FP16 训练：基准
- 纯 FP8 训练：快 6.2 倍，但精度下降 0.5%（不可接受）
- Transformer Engine（混合）：快 5.8 倍，精度下降<0.1%

### 精度降低的极限

能不能继续降低到 FP4？

**FP4 的挑战**：

```
FP4（1符号+2指数+1尾数）：
• 只能表示16个不同的值！
• 在[-1,1]范围内：{-1, -0.5, -0.25, 0, 0.25, 0.5, 1}
• 训练时，梯度更新可能小于最小间隔（无效更新）
```

实验显示：

- 小模型（<1B 参数）：FP4 训练勉强可行，精度下降 5-10%
- 大模型（>10B 参数）：FP4 训练不收敛

可能的方向：

- **FP6**：平衡精度和性能（理论 10.7 倍加速）
- **自适应精度**：不同层用不同精度（重要层用 FP8，其他用 FP4）
- **量化感知训练**：在训练时就模拟低精度（让模型"适应"低精度）

**本章结论**：AI 的"模糊数学"特性让精度换速度成为可能。从 FP32→FP8 获得了 8 倍理论加速，实际应用中达到 5-6 倍。但继续降低面临收敛性挑战，FP8 可能是训练的精度下限。

---

## 第四章：内存墙的突破——HBM 的 3D 革命

### 3TB/s 带宽从哪来？

H100 的 Tensor Core 峰值性能是 1513 TFLOPS（FP8）。假设每次 FMA 需要读取 2 个输入（a 和 b）、写入 1 个输出，那么：

```
内存需求：
• 每秒操作数：1513 × 10¹² FMA
• 每次FMA读写：2 input × 1 byte (FP8) + 1 output × 2 bytes (FP16累加) = 4 bytes
• 总内存带宽需求：1513 × 10¹² × 4 bytes = 6 TB/s
```

但 H100 的 HBM3"只有"3 TB/s。为什么够用？因为 Tensor Core 内部有数据复用（前面讲的脉动阵列）。但即使考虑复用，3 TB/s 仍是必需的。

问题是：传统的 GDDR 内存只有 1 TB/s。如果用 GDDR6X，Tensor Core 将有 67%的时间在等待数据，性能直接腰斩。

### GDDR 的物理限制

让我们看看为什么 GDDR 到达了极限。

**GDDR 的连接方式**（以 RTX 4090 为例）：

```
GPU芯片          PCB走线          GDDR6X芯片
┌──────┐         ~~~~~~~~         ┌──────┐
│      │ <---5-10 cm铜线---> │      │
│ Core │         ~~~~~~~~         │ Memory│
└──────┘                          └──────┘

物理限制：
• 走线长度：5-10 cm（太长会有信号衰减）
• 寄生电容：~10 pF per 走线（限制频率）
• 串扰：相邻走线互相干扰（限制密度）
```

这导致三大限制：

**限制 1：接口宽度**

- GDDR6X 最多支持 384-bit 接口（48 个引脚 × 8 个内存芯片）
- 为什么不能更宽？因为 GPU 封装的边缘引脚数量有限（约 3000 个引脚，其中一半给电源、PCIe、DisplayPort 等）
- 再增加引脚：封装成本指数上升，且 PCB 布线变得极其复杂

**限制 2：频率**

- GDDR6X 达到 21 Gbps（千兆比特每秒）
- 进一步提升频率：信号完整性问题（反射、串扰、电磁干扰）
- 需要更复杂的信号处理电路，功耗大幅上升

**限制 3：能效**

- 当前功耗：~40 mW per GB/s
- 高速信号需要强驱动电流，推动 10 pF 的电容到 21 GHz，功耗是平方关系

**GDDR6X 的极限**：

```
带宽 = 接口宽度 × 频率 / 8
     = 384-bit × 21 Gbps / 8
     = 1,008 GB/s

这是2D平面布局的理论极限
```

要突破，必须改变连接的**物理维度**。

### HBM 的 3D 堆叠方案

HBM（High Bandwidth Memory）的核心思想：**如果不能让 GPU 和内存靠得更近（平面上），那就把内存叠到 GPU 上面**。

**HBM3 的物理结构**：

```
侧面视图（单个HBM Stack）：

  ┌──────────────┐  ↑
  │ DRAM Die 12  │  │
  ├──────────────┤  │
  │ DRAM Die 11  │  │
  ├──────────────┤  │
  │     ...      │  │ 12层垂直堆叠
  ├──────────────┤  │ 通过TSV连接
  │ DRAM Die 2   │  │
  ├──────────────┤  │
  │ DRAM Die 1   │  ↓
  ├──────────────┤
  │ Base Die     │ ← 逻辑控制层
  └──────────────┘
        ↓
  ┌──────────────┐
  │ Interposer   │ ← 硅中介层（2.5D封装）
  │  (连接层)     │
  └──────────────┘
        ↓
  ┌──────────────┐
  │  GPU Die     │
  └──────────────┘
```

**关键技术：TSV（Through-Silicon Via，硅通孔）**

TSV 是在硅片上钻出垂直的孔，用铜填充，允许信号穿过芯片：

```
TSV横截面（放大1000倍）：
┌─────────────────┐
│   硅片（Die）    │
│                 │
│    ╔═════╗      │ ← TSV孔
│    ║ Cu  ║      │   直径：5-10 微米
│    ║铜填充║      │   高度：50-100 微米
│    ╚═════╝      │
├─────────────────┤
│ 绝缘层 (SiO₂)   │ ← 防止漏电
└─────────────────┘

单个HBM Stack的TSV数量：
• 数据TSV：1024个（总线宽度）
• 电源TSV：约200个
• 地线TSV：约200个
• 总计：约1400个TSV
```

**TSV vs PCB 走线的对比**：

| 特性     | PCB 走线      | TSV           | 改善倍数  |
| -------- | ------------- | ------------- | --------- |
| 长度     | 5-10 cm       | 50-100 μm     | **1000×** |
| 寄生电容 | ~10 pF        | ~0.05 pF      | **200×**  |
| 能耗     | ~40 mW/(GB/s) | ~10 mW/(GB/s) | **4×**    |
| 接口密度 | 48 pins/chip  | 1024 pins/die | **21×**   |
| 最高频率 | 21 Gbps       | 50+ Gbps      | **2.4×**  |

关键洞察：**把 10 cm 缩短到 50 μm，物理规律完全不同**。就像从"用绳子拉货车"（PCB 走线）变成"用刚性杆推货车"（TSV）——效率天差地别。

### H100 的 HBM3 子系统

H100 使用 5 个 HBM3 Stack。为什么是 5 个而不是 4 个或 6 个？

**直白版类比**：

```
想象GPU是一个工厂，HBM Stack是原料仓库：

工厂布局（俯视图）：
        仓库2   仓库1
           ↓      ↓
    [ ]  [ ]  [工厂]  [ ]  [ ]
           ↑      ↑
        仓库3   仓库4   仓库5

为什么5个仓库？
• 4个：对称放在四周，但GPU太大，四周放不下足够容量
• 6个：可以做到，但Interposer面积成本暴增30%
• 5个：折中方案（4个主仓库 + 1个中央仓库）

类比现实：
• 每个仓库 = 一栋16层的楼（12层存货 + 4层电梯/控制）
• 每层存货 = 2个货架，共24个货架
• 每个货架容量 = 0.67 GB（24货架 × 0.67 = 16 GB）
```

**技术细节**：单个 Stack 怎么做到 16 GB？

```
垂直结构：
┌──────────────┐
│ DRAM Die 12  │ }
├──────────────┤  } 12层数据层
│ DRAM Die 11  │ }   每层容量：16 Gb = 2 GB
├──────────────┤     (注意：bit vs Byte)
│     ...      │
├──────────────┤
│ DRAM Die 1   │ }
├──────────────┤
│ Base Die     │ ← 控制层（不存数据）
└──────────────┘

为什么不是12层×2GB=24GB？
• 因为12层每层是16 Gb（gigabit），不是GB（gigabyte）
• 16 Gb = 2 GB（8 bit = 1 Byte）
• 所以：12层 × 2 GB/层 = 24 GB
• 但实际只用了8层？不，实际是12层但每层容量缩小了

真相：制造灵活性
• 厂商可以选择不同配置：
  - 12层 × 1.33 GB = 16 GB（H100的选择）
  - 8层 × 2 GB = 16 GB（成本更低，但带宽略降）
  - 16层 × 1 GB = 16 GB（更薄，散热更好）
```

**5 个 Stack 的简单算术**：

| 指标              | 单个 Stack | 5 个 Stack | 对比 GDDR6X    |
| ----------------- | ---------- | ---------- | -------------- |
| **容量**          | 16 GB      | 80 GB      | 24 GB (3.3×)   |
| **接口宽度**      | 1024-bit   | 5120-bit   | 384-bit (13×)  |
| **速度**          | 5.2 Gbps   | 5.2 Gbps   | 21 Gbps        |
| **单 Stack 带宽** | 665 GB/s   | -          | -              |
| **总带宽**        | -          | 3000 GB/s  | 1008 GB/s (3×) |
| **功耗**          | 6W         | 30W        | 40W            |

**为什么 GDDR 更高速度（21 Gbps）但带宽更低？**
答案：**接口宽度碾压速度**。HBM 用 13 倍宽的"高速公路"，即使每辆车慢一些，总吞吐量仍然高 3 倍。

### 内存组织与访问效率

HBM 虽然带宽高，但不意味着任何访问模式都快。

**直白版类比**：

```
HBM = 8条独立的高速公路（Pseudo Channel）
每条公路有8个出口（Bank）
每个出口有一个临时停车场（Row Buffer，8 KB）

访问规则：
✅ 好模式：顺序访问同一出口 → 车直接从停车场出发（快）
❌ 坏模式：随机换出口 → 每次都要重新停车、换出口（慢）
❌ 最坏：多辆车同时要去同一出口 → 排队等待（更慢）
```

**访问效率对比**（H100 HBM3 实测）：

| 访问模式      | 延迟    | 有效带宽  | 效率 | 类比               |
| ------------- | ------- | --------- | ---- | ------------------ |
| 随机 4 字节   | ~400 ns | 50 GB/s   | 1.7% | 每次只拿一个螺丝钉 |
| 随机 64 字节  | ~400 ns | 400 GB/s  | 13%  | 每次拿一盒零件     |
| 顺序 128 字节 | ~100 ns | 2000 GB/s | 67%  | 流水线传送         |
| 完美交错访问  | ~100 ns | 3000 GB/s | 100% | 8 条高速公路全开   |

**关键洞察**：即使有 3 TB/s 的理论带宽，随机访问只有 50 GB/s（比 GDDR6X 还慢！）。HBM 的价值在于**连续大块传输**。

**优化的 kernel 如何做**（如 cuBLAS、FlashAttention）：

1. 矩阵按行/列连续存储（确保顺序访问）
2. 访问时让 8 个 Channel 并行工作（Bank 交错）
3. 重用 Shared Memory，减少 HBM 往返次数

### HBM vs GDDR：完整对比

| 维度           | GDDR6X (RTX 4090) | HBM3 (H100)               | 优势方       |
| -------------- | ----------------- | ------------------------- | ------------ |
| **带宽**       | 1008 GB/s         | 3000 GB/s                 | HBM **3×**   |
| **延迟**       | ~80 ns            | ~100 ns                   | GDDR         |
| **容量**       | 24 GB             | 80 GB                     | HBM **3.3×** |
| **功耗**       | 40W               | 30W                       | HBM          |
| **能效**       | 25 GB/s per W     | 100 GB/s per W            | HBM **4×**   |
| **成本**       | $$                | $$$$                      | GDDR         |
| **封装复杂度** | 低（2D PCB）      | 高（2.5D Interposer+TSV） | GDDR         |

**为什么游戏卡不用 HBM？**

1. **成本**：HBM 需要 TSV 工艺和 Interposer，制造成本是 GDDR 的 3-4 倍
2. **游戏不需要极致带宽**：渲染 4K@60fps，带宽需求约 500 GB/s，GDDR6X 的 1 TB/s 足够
3. **容量优先**：游戏玩家要的是"大显存"（存高分辨率纹理），而非"快显存"

**数据中心为什么必须 HBM？**

训练大模型的瓶颈在于"参数搬运"：

```
GPT-3 (175B参数，FP16)：
• 模型大小：175B × 2 bytes = 350 GB
• 单次前向传播：需要加载所有参数一次
• 使用GDDR6X（1 TB/s）：350 GB / 1 TB/s = 0.35秒
• 使用HBM3（3 TB/s）：350 GB / 3 TB/s = 0.12秒

每个训练step节省：0.23秒
训练100万step：节省230,000秒 = 64小时

对于千卡集群，HBM vs GDDR的差异 = 数月训练时间
```

**本章结论**：HBM 通过 3D 堆叠突破了 2D 布局的物理极限，带来 3 倍带宽和 4 倍能效。这是释放 Tensor Core 和 FP8 潜力的关键——没有 HBM，Tensor Core 会饿死在等待数据的路上。

---

## 第五章：三场革命的协同效应

### 性能提升的数学分解

为什么从 V100 到 H100 的训练速度能提升数十倍？

答案在于硬件升级（如更多 SM 和 Tensor Core）、精度降低（如 FP8）和内存优化（如 HBM3）的协同。

但基准不同会影响对比 —— ResNet-50 是 50 倍（8 小时 vs 10 分钟），而 GPT-3 175B 更复杂，受模型规模和数据集影响。

以下基于 NVIDIA 报告、MLPerf v3.0/v4.0 和第三方（如 Lambda Labs、MosaicML）实测数据，统一到 GPT-3 175B 模型（完整 300B tokens 训练，~1,000 GPUs 集群投影）。

#### 训练时间对比

| 配置                     | V100 时间（~1,024 GPUs） | A100 时间（~1,024 GPUs，参考） | H100 时间（~1,024-3,584 GPUs） | 加速比 (vs V100)                | 解释                                                                                |
| ------------------------ | ------------------------ | ------------------------------ | ------------------------------ | ------------------------------- | ----------------------------------------------------------------------------------- |
| **FP32 基准** (完整精度) | ~51 天                   | ~34-36 天                      | ~10-15 天                      | **~3-5×**                       | V100 默认 FP32                                                                      |
| **FP16 混合精度**        | ~40 天                   | ~28 天                         | ~4-5 天                        | **~8-10×**                      | 统一 FP16 反映硬件贡献（如 Tensor Core 改进）                                       |
| **FP8 优化** (基于 FP16) | 不支持                   | 不支持                         | ~1-2 天                        | **额外 2-3× <br> (总 ~25-30×)** | H100 专属 FP8（Transformer Engine），实测 2-3× 加速（MosaicML 测试）。总 ~25-30× 。 |

**加速比分解**：

```
总加速比 = SM × Tensor性能 × FP8 × HBM × Cache × 其他
         = 1.65 × 3.0 × 2.5 × 1.5 × 1.15 × 1.4
         ≈ 24.8×
```

**完整分解**：

```
总加速比 ≈ 24.8×
├─ 更多SM
│  └─ 132个 vs 80个 → 1.65×
│
├─ Tensor Core性能提升
│  ├─ 频率提升：1.53 GHz → 1.755 GHz → 1.15×
│  ├─ 每个Tensor Core架构优化（4th-gen vs 1st-gen） → 1.3×
│  ├─ 更多SM（重叠因子） → 1.65×
│  └─ 乘积 ≈ 3.0×
│
├─ FP8相对FP16
│  ├─ 计算密度翻倍（8 bits vs 16 bits） → 2×
│  └─ 额外并行优化（等效矩阵尺寸增加，Transformer Engine） → 1.25×（总2.5×）
│
├─ HBM3带宽提升
│  ├─ 理论带宽：900 GB/s → 3000 GB/s → 3.33×
│  └─ 实际贡献（考虑不同操作受限程度，如Attention内存密集） → 1.5×
│
├─ L2 Cache增大
│  ├─ 6 MB → 50 MB → 8.3×
│  └─ 实际贡献（减少HBM访问，适用于中间结果） → 1.15×
│
└─ 其他优化
   ├─ NVLink 4.0（多卡通信，提升AllReduce效率） → 1.2×
   ├─ Transformer Engine（动态精度，避免转换开销） → 1.1×
   ├─ Thread Block Cluster（线程组织，减少同步） → 1.05×
   └─ 乘积 ≈ 1.4×
```

为什么计算出的总加速比只有 24.8×，而实测约 25-30×

差异来自**协同效应**——因素之间不是简单相乘，而是相互放大。

### 协同效应：为什么 1+1+1>3

为什么这些因子叠加后超出乘积？因为它们消除重叠瓶颈：
FP8 减少数据量，让 HBM3 更有效；
更大 Cache 避免写回拖累 Tensor Core。

让我们用 Self-Attention 的 QK^T 矩阵乘法来说明协同效应（batch=64，seq_len=512；基于 NVIDIA 基准数据）。

**V100 vs H100 完整对比表**：

| 步骤                    | V100 (FP16)                               | H100 (FP8)                                    | 改进来源                          |
| ----------------------- | ----------------------------------------- | --------------------------------------------- | --------------------------------- |
| **1. 加载 Q 矩阵**      | 2 MB / 900 GB/s<br>= 2.2 μs               | 1 MB / 3000 GB/s<br>= 0.33 μs                 | HBM3 (3.3×) + FP8 (2×) = **6.6×** |
| **2. 加载 K 矩阵**      | 2 MB / 900 GB/s<br>= 2.2 μs               | 1 MB / 3000 GB/s<br>= 0.33 μs                 | 同上 **6.6×**                     |
| **3. Tensor Core 计算** | 2.1 GFLOP / 125 TFLOPS<br>= 16.8 μs       | 2.1 GFLOP / 1513 TFLOPS<br>= 1.4 μs           | Tensor 性能 **12×**               |
| **4. 写回结果**         | 16 MB / 900 GB/s<br>= 17.8 μs<br>(瓶颈！) | 32 MB 装入 L2 Cache<br>= 0.8 μs<br>(不回 HBM) | L2 (8.3×) **22×**                 |
| **总时间**              | 39 μs                                     | 2.86 μs                                       | **13.6×**                         |

**协同效应拆解**：

| 单项技术               | 单独使用       | 协同后实际                          | 倍增因子     |
| ---------------------- | -------------- | ----------------------------------- | ------------ |
| HBM3 (3.3× 带宽)       | 理论快 3.3×    | 输入加载：2.2→0.67 μs<br>**3.3×**   | 接近理论     |
| FP8 (精度减半)         | 理论快 2×      | 配合 HBM3：0.67→0.33 μs<br>**6.6×** | 1+1>2 ✓      |
| Tensor Core (12× 性能) | 理论快 12×     | 计算时间：16.8→1.4 μs<br>**12×**    | 释放算力 ✓   |
| L2 Cache (8.3× 容量)   | 理论减 8× 访问 | 消除写回瓶颈<br>**22×**             | 消除阻塞点 ✓ |

**为什么 1+1+1>3？**

1. **FP8 × HBM3 = 6.6 倍**（不是 3.3×2=6.6，而是消除了双重瓶颈）

   - FP8 让数据量减半
   - HBM3 让传输快 3.3 倍
   - 两者配合：内存时间从 4.4μs → 0.66μs

2. **L2 Cache 消除了最大瓶颈**

   - V100：写回 17.8μs 占总时间 46%（最大瓶颈）
   - H100：结果装入 L2，瓶颈消失
   - 如果没有 L2：即使前 3 步再快，也被写回拖慢

3. **Tensor Core 终于不饿死**
   - V100：Tensor Core 算完（16.8μs）还要等写回（17.8μs）
   - H100：Tensor Core 算完，后续步骤立即跟上
   - 利用率：40% → 95%

**关键结论**：三者缺一，性能至少打五折。

### 不同场景的实际加速比

并非所有 workload 都能获得 25-30× 提升。关键看是否命中三大优化（密集矩阵、大 batch、Transformer）。

**场景速查表**：

| Workload                       | V100 性能 | H100 性能 | 加速比  | 命中的优化                 | 瓶颈            |
| ------------------------------ | --------- | --------- | ------- | -------------------------- | --------------- |
| **大模型训练**<br>(GPT-3)      | 能用      | 理想      | **30×** | Tensor ✓<br>FP8 ✓<br>HBM ✓ | -               |
| **大模型推理**<br>(batch≥32)   | 慢        | 快        | **28×** | 同上                       | 通信开销减少    |
| **小 batch 推理**<br>(batch=1) | 不划算    | 可行      | **8×**  | FP8 ✓<br>Tensor △          | 矩阵太小        |
| **CNN 训练**<br>(ResNet)       | 够用      | 更快      | **15×** | Tensor ✓<br>HBM △          | im2col 开销 10% |
| **GNN 训练**<br>(图神经网络)   | 慢        | 较快      | **5×**  | HBM △                      | 稀疏+随机访问   |
| **传统 HPC**<br>(需 FP64)      | -         | 不适合    | **<2×** | -                          | 无精度优化      |

**典型案例分析**：

**✅ 最佳：GPT-3 训练（30 倍）**

```
为什么这么快？
• 90%时间在矩阵乘法 → Tensor Core利用率95%
• Batch=64，seq=2048 → 大矩阵充分发挥HBM
• 混合精度收敛良好 → FP8无损失
• L2装得下中间结果 → 减少50% HBM访问
```

**△ 中等：CNN 训练（15 倍）**

```
为什么不到41倍？
• 卷积→矩阵需im2col变换（占10%时间，无法加速）
• 矩阵形状不规则（如3×3卷积核）→ Tensor Core利用率70%
• BatchNorm、ReLU等占5%时间 → 用CUDA Core
```

**❌ 较差：图神经网络（5 倍）**

```
为什么这么慢？
• 邻接矩阵稀疏（>90%零元素）→ Tensor Core浪费
• 根据图结构随机访问 → HBM效率仅15%
• FP8可用但不是瓶颈
```

**关键洞察**：H100 是为**密集矩阵 + 大 batch + Transformer**量身定制。偏离这个 sweet spot，加速比迅速衰减。

---

## 第六章：未来的挑战与方向

### 三大技术接近极限

我们刚刚见证了 GPU 性能的黄金十年（2012-2022）：从 AlexNet 到 ChatGPT，性能提升了约 1000 倍。但物理定律在前方设置了墙壁。

**极限 1：Tensor Core 的面积占比**

当前 H100：

- Tensor Core：15%芯片面积
- CUDA Core：60%芯片面积
- 其他（缓存、互联等）：25%

能否继续增加 Tensor Core？

```
假设Tensor Core提升到30%：
• 需要减少CUDA Core到45%
• 但神经网络仍有10%计算是非矩阵操作
• CUDA Core不足会导致这10%变成瓶颈
• 实际加速比：1.3×（而非理论的2×）

投入产出比递减
```

**极限 2：精度降低的收敛性**

| 精度 | 训练可行性    | 推理可行性        | 限制因素              |
| ---- | ------------- | ----------------- | --------------------- |
| FP16 | ✅ 稳定       | ✅ 稳定           | -                     |
| FP8  | ✅ 需混合精度 | ✅ 稳定           | 小模型(<1B)可能不收敛 |
| FP6  | ⚠️ 实验中     | ✅ 可用           | 大模型训练困难        |
| FP4  | ❌ 不收敛     | ⚠️ 精度下降 5-10% | 梯度消失/爆炸         |

当前共识：**FP8 可能是训练的精度下限**，FP4 只能用于推理且需要特殊量化技巧。

**极限 3：HBM 的成本与容量**

HBM3 的挑战：

- **成本**：单个 Stack 约$200，5 个 Stack = $1000（占 GPU 成本 30-40%）
- **容量**：单 Stack 最大 16 GB，5 Stack = 80 GB
- **散热**：堆叠 12 层 DRAM，中间层散热困难

扩展的矛盾：

- 大模型需要更大显存（GPT-4 可能需要 200+ GB）
- HBM3e 下一代：96 GB（6 Stack × 16 GB），但成本$1500+
- 再往上？物理上很难超过 128 GB

### 新的突破方向

既然三大技术接近极限，未来的提升从哪来？

**方向 1：稀疏计算**

关键发现：神经网络的 90%权重接近零。

```
GPT-3的权重分布（实测）：
• 60%的权重 < 0.01（几乎无贡献）
• 30%的权重 < 0.1
• 只有10%的权重真正重要

如果跳过零权重的计算：理论加速10倍
```

H100 已经支持**结构化稀疏（2:4 模式）**：

- 每 4 个权重中，强制 2 个为零
- 硬件跳过零元素的计算
- 实际加速：1.5 倍（理论 2 倍，但有 overhead）

未来：**动态稀疏**

```
理想的稀疏Tensor Core：
1. 输入矩阵带稀疏元数据（哪些元素非零）
2. 硬件实时解析元数据
3. 只计算非零元素

挑战：
• 元数据解析需要额外电路
• 不规则稀疏导致负载不平衡
• 压缩格式（CSR、COO）的解码延迟

如果解决：90%稀疏 → 理论10倍加速
```

**方向 2：3D 堆叠 GPU 芯片**

当前 GPU 是单层芯片。如果垂直堆叠多层？

```
3D堆叠GPU（概念）：
┌──────────────┐
│ Compute Die 2│ ← 全是Tensor Core
├──────────────┤
│ Compute Die 1│ ← 全是CUDA Core
├──────────────┤
│ Cache Die    │ ← 128 MB L2/L3 Cache
├──────────────┤
│ HBM Die      │ ← HBM直接堆叠在GPU上
└──────────────┘

优势：
• 层间距离<10 μm（比Interposer短100倍）
• 带宽提升10倍（理论30 TB/s）
• 不同层可以用不同工艺（如Cache层用老制程降低成本）

挑战：
• 散热：中间层无法直接散热，需要"热通道"
• 良率：4层堆叠，单层90%良率 → 总良率65%
• TSV密度：需要10,000+个连接
```

NVIDIA 已在实验室测试 3D 堆叠。

**方向 3：光学互联**

当 GPU 集群达到 10,000+卡时，电互联的功耗不可承受。

```
NVLink 4.0（电互联）：
• 18个链路 × 75 GB/s = 1.35 TB/s
• 功耗：~50W
• 距离限制：<10 m

光学NVLink（硅光子）：
• 理论带宽：10+ TB/s
• 功耗：~5W（效率提升10倍）
• 距离：可达数公里（数据中心内任意位置）

工作原理（简化）：
电信号 → 激光调制器 → 光纤 → 光电探测器 → 电信号
```

苹果、Intel、NVIDIA 都在研发硅光子技术。

**方向 4：Processing-in-Memory（PIM）**

终极方案：把计算单元直接放进 HBM。

```
HBM-PIM（概念）：
┌─────────────────────────────────┐
│  DRAM Array（存储）              │
├─────────────────────────────────┤
│  PIM Core（计算）                │
│  ├─ 64个简化ALU                 │
│  ├─ 本地寄存器4 KB               │
│  └─ 只能做简单操作（加法、ReLU）  │
└─────────────────────────────────┘

适合的操作：
• Softmax（大量exp和sum）
• LayerNorm（统计和归一化）
• 激活函数（ReLU、GeLU）

不适合的操作：
• 矩阵乘法（太复杂，PIM Core算力不够）

效果：
• 减少50%的GPU↔HBM数据传输
• Transformer中非矩阵操作加速5-10倍
```

三星已推出 HBM-PIM 原型，但 NVIDIA 尚未商用。

### 专用化 vs 通用化的未来

过去 10 年，GPU 走向专用化：从通用 CUDA Core 到专用 Tensor Core。但专用化有代价。

**专用化的困境**：

```
2022年，Stable Diffusion发布（扩散模型）：
• 核心操作：U-Net卷积 + Cross-Attention
• H100的Transformer Engine针对标准Transformer优化
• 但U-Net的结构不同，Transformer Engine命中率只有60%

2023年，Mamba模型发布（状态空间模型）：
• 不再是Transformer架构
• H100的Tensor Core仍有效，但Transformer Engine无用
```

问题：**算法演进速度 > 硬件迭代速度**。如果每年都有新架构，GPU 无法跟上。

可能的折中方案：**可重构加速器**

```
可重构Tensor Core（推测）：
┌────────────────────────────────────┐
│  可编程Systolic Array               │
│  ├─ 矩阵尺寸可配置：8×8 / 16×16 / 32×32│
│  ├─ 数据类型可配置：FP4-FP64        │
│  ├─ 运算模式可配置：                 │
│  │  ├─ 标准矩阵乘法                 │
│  │  ├─ 卷积（直接模式，无需im2col）   │
│  │  ├─ FFT（快速傅里叶变换）         │
│  │  └─ 稀疏矩阵运算                 │
├────────────────────────────────────┤
│  微码存储器（128 KB）                │
│  └─ 存储不同workload的配置          │
├────────────────────────────────────┤
│  JIT编译器（硬件）                   │
│  └─ 运行时分析kernel并生成微码      │
└────────────────────────────────────┘

工作方式：
1. 检测输入kernel的特征（矩阵大小、稀疏度等）
2. 从微码库选择最优配置
3. 如果没有匹配的，JIT编译器生成新配置
4. 重构Systolic Array的连接方式

类比：像FPGA，但延迟更低（<1微秒重构）
```

这种设计结合了专用和通用的优点：

- 对于已知 workload（Transformer），达到专用硬件 90%性能
- 对于新 workload（Mamba），仍能达到 60-70%性能
- 代价：额外 20%的芯片面积

---

## 总结

### 核心要点

从 V100 到 H100 的 50 倍提升，来自三场革命的叠加：

1. **Tensor Core（15 倍效率）**：用专用矩阵电路替代通用 CUDA Core，通过脉动阵列实现数据复用，能效比提升 20 倍。

2. **精度降低（6 倍实际）**：利用神经网络的"模糊数学"特性，从 FP32→FP8，理论 8 倍加速，实际 6 倍（考虑混合精度开销）。

3. **HBM3 内存（3 倍带宽）**：通过 3D 堆叠和 TSV 技术，突破 2D 走线限制，带宽从 900 GB/s 提升到 3000 GB/s，能效提升 4 倍。

4. **协同效应（非线性叠加）**：三者缺一不可——没有 HBM，Tensor Core 会饿死；没有 FP8，HBM 带宽浪费；没有 Tensor Core，前两者无用武之地。

5. **未来挑战**：三大技术接近物理极限，下一个 10 年的提升将来自稀疏计算、3D 堆叠、光互联、PIM 等新方向。

### 关键数据速查

| 指标            | V100 (2017)        | H100 (2022)       | 提升       |
| --------------- | ------------------ | ----------------- | ---------- |
| **制程**        | TSMC 16nm          | TSMC 4nm          | 4× 密度    |
| **晶体管**      | 211 亿             | 800 亿            | 3.8×       |
| **SM 数量**     | 80                 | 132               | 1.65×      |
| **Tensor 性能** | 125 TFLOPS (FP16)  | 1513 TFLOPS (FP8) | 12×        |
| **CUDA 性能**   | 15.7 TFLOPS (FP32) | 60 TFLOPS (FP32)  | 3.8×       |
| **内存**        | 16 GB HBM2         | 80 GB HBM3        | 5× 容量    |
| **内存带宽**    | 900 GB/s           | 3000 GB/s         | 3.3×       |
| **功耗**        | 300W               | 700W              | 2.3×       |
| **能效**        | 0.42 TFLOPS/W      | 2.16 TFLOPS/W     | 5.1×       |
| **价格**        | $8,000             | $30,000           | 3.75×      |
| **训练速度**    | 基准               | 30×               | GPT-3 场景 |

### 适用场景指南

**最适合 H100 的场景**（>30 倍加速）：

- 大语言模型训练（GPT、LLaMA）
- Transformer 推理（batch>32）
- 密集矩阵运算（BERT、ViT）

**较适合 H100 的场景**（10-20 倍加速）：

- CNN 训练（ResNet、EfficientNet）
- 中等 batch 推理（8-32）
- 传统 NLP（RNN、LSTM）

**不太适合 H100 的场景**（<5 倍加速）：

- 小 batch 推理（<4）
- 稀疏/图神经网络
- 科学计算（需要 FP64）
- 传统计算机视觉（非深度学习）

### 展望

AI 芯片的黄金时代还未结束。虽然 Tensor Core、精度、HBM 三大技术接近极限，但新的方向正在打开：

- **2024-2025**：FP6 精度、结构化稀疏 2:4、HBM3e（4.8 TB/s）
- **2026-2027**：3D 堆叠 GPU、光学互联、动态稀疏
- **2028-2030**：HBM-PIM、可重构加速器、新型内存架构

10 年后的 GPU，可能看起来和今天完全不同。但核心的设计哲学不会变：**找到应用的瓶颈，用专用硬件突破，用架构创新协同放大**。

这就是从 V100 到 H100 的故事——不是简单的"更多核心、更高频率"，而是重新思考"什么是 AI 计算的本质"，并用硅实现这个答案。

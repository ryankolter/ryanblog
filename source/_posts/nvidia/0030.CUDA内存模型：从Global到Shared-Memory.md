---
title: CUDA内存模型：从Global到Shared Memory
date: 2025-10-21 20:59:43
categories:
  - NVIDIA
top_order: 30
---

GPU 有 10,000 个计算核心，为什么实际吞吐常常只是理论峰值的 10-30%？

答案不在计算，而在内存。

<!--more-->

---

## 1. 内存墙 —— GPU 的隐藏瓶颈

### 1.1 算力过剩 vs 带宽不足

想象一个工厂：1000 个工人在流水线上组装零件，但仓库只有 1 个窄门，每秒只能通过 10 箱原材料到达车间。

结果是什么？工人们大部分时间在等原材料，而不是在工作。

瓶颈不是工人不够快，而是门太窄。

GPU 的情况完全一样。看 RTX 4090 的数据：

| 指标     | 数值        |
| -------- | ----------- |
| 算力峰值 | 82.6 TFLOPS |
| 内存带宽 | 1008 GB/s   |

#### 单次运算的例子

以最简单的向量加法为例：`C[i] = A[i] + B[i]`

```
单次运算的需求：
├─ 读取：A[i] (4字节) + B[i] (4字节) = 8字节
├─ 计算：1次浮点加法
└─ 写入：C[i] (4字节)
──────────────────────────────────────
总计：需要搬运 12字节，完成 1次运算
```

算一下带宽限制下，每秒能做多少次这样的运算：

```
带宽限制的算力：
1008 GB/s ÷ 12 字节/次 = 84 G次/秒 = 84 GFLOPS

对比：
├─ 带宽能支撑：84 GFLOPS
└─ 芯片能计算：82.6 TFLOPS = 82,600 GFLOPS
──────────────────────────────────────
利用率：84 / 82,600 = 0.1%
```

**核心矛盾**：99.9%的计算单元在等内存数据，只有 0.1%在真正工作，这里 GPU 的计算单元比内存快了 1000 倍。

这就是**内存墙**：即使有 10,000 个核心，如果数据供应不上，大部分核心也只能闲置。

### 1.2 GPU 如何应对：分层内存系统

既然搬数据这么慢，GPU 的设计思路是什么？

答案是**局部性原理**：把常用的数据放在快速但小的存储里，不常用的数据放在慢速但大的存储里。

这和人类处理文件的方式一样：

```
正在处理的文件 → 摊在桌上（最快）
本周要用的文件 → 放办公室书柜（快）
历史档案 → 存档案室（慢但容量大）
```

GPU 也设计了多层内存，速度差异巨大：

| 内存类型          | 容量      | 延迟         | 带宽      | 位置      |
| ----------------- | --------- | ------------ | --------- | --------- |
| **Register**      | 256 KB/SM | 1 周期       | ~100 TB/s | SM 内     |
| **Shared Memory** | 128 KB/SM | ~30 周期     | ~10 TB/s  | SM 内     |
| L2 Cache          | 96 MB     | ~200 周期    | -         | 芯片上    |
| **Global Memory** | 24 GB     | 400-600 周期 | 1 TB/s    | 显存(HBM) |

关键数字：

- Register 比 Global Memory 快**600 倍**
- 但容量小 **10 万倍**

如果能把数据从 Global Memory 搬到 Shared Memory 或 Register，就能大幅提升性能。

```
假设某个数据需要被访问 100 次：
├─ 全部从 Global 读取：600 周期 × 100 = 60,000 周期
└─ 搬到 Shared 后访问：600 周期（搬运）+ 30 周期 × 100（访问）= 3,600 周期
────────────────
加速比：16.7×
```

---

## 2. Global Memory —— 合并访问是生命线

为什么相邻线程读相邻地址，能快 32 倍？

### 2.1 内存事务的真相

GPU 访问 Global Memory 不是一个字节一个字节读的，而是以**128 字节**为单位。这被称为一次**内存事务**（Memory Transaction）。

想象超市结账：

```
情况 1：非合并访问
32 个顾客排队，每人买 1 件商品，分别结账。收银员要扫描 32 次，依次收款找零。

情况 2：合并访问
这 32 个顾客一起结账，收银员一次扫描所有商品，统一收款找零。
```

第二种情况快 32 倍，因为减少了找零、刷卡等固定开销。

GPU 的内存访问也是如此:

```
一个 Warp 有 32 个线程，它们会同时发起内存请求。
GPU 的内存控制器会分析这 32 个请求的地址：

- 如果地址连续：32 个线程访问的是连续的 128 字节（32 个 float） → 合并成 1 次事务
- 如果地址分散：可能需要 32 次独立的内存事务
```

### 2.2 合并 vs 非合并访问

假设有一个数组，256 个线程访问它：

```c++
__global__ void example(float* array) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    // 访问模式1：连续访问
    float val = array[tid];

    // 访问模式2：跨步访问
    float val = array[tid * 32];
}
```

**模式 1：连续访问**

前 32 个线程（第一个 Warp）访问：

- Thread 0 → array[0]
- Thread 1 → array[1]
- Thread 2 → array[2]
- ...
- Thread 31 → array[31]

这 32 个地址刚好是连续的 128 字节（32 × 4 字节），可以合并成**1 次内存事务**。

**模式 2：跨步访问**

前 32 个线程访问：

- Thread 0 → array[0]
- Thread 1 → array[32]
- Thread 2 → array[64]
- ...
- Thread 31 → array[992]

每两个地址之间相隔 128 字节，这 32 个地址分散在 32 个不同的 cache line 里，需要**32 次内存事务**。

真实测试结果（RTX 4090，向量加法）：

| 访问模式 | 带宽     | 峰值利用率 | 相对速度  |
| -------- | -------- | ---------- | --------- |
| 连续访问 | 920 GB/s | 91%        | **1.0×**  |
| 步长=2   | 485 GB/s | 48%        | 0.53×     |
| 步长=4   | 250 GB/s | 25%        | 0.27×     |
| 步长=32  | 28 GB/s  | 2.8%       | **0.03×** |

步长为 32 时，性能只有合并访问的 3%——慢了**32.8 倍**。

### 2.3 什么情况会破坏合并？

以下是常见的访问模式及其效率：

| 访问模式    | 代码示例      | Warp 内访问的地址 | 事务数 | 效率    |
| ----------- | ------------- | ----------------- | ------ | ------- |
| **连续**    | `arr[tid]`    | 0,1,2,...,31      | 1      | 100% ✅ |
| **步长=2**  | `arr[tid*2]`  | 0,2,4,...,62      | 2      | 50%     |
| **步长=16** | `arr[tid*16]` | 0,16,32,...,496   | 16     | 6%      |
| **步长=32** | `arr[tid*32]` | 0,32,64,...,992   | 32     | 3% ❌   |
| **随机**    | `arr[rand()]` | 随机 32 个地址    | ~32    | 3% ❌   |
| **未对齐**  | `arr[tid+1]`  | 1,2,3,...,32      | 2      | 50%     |

**关键规律**：Warp 内 32 个线程访问的地址越分散，需要的内存事务越多，效率越低。

### 2.4 访问模式的影响

回到最简单的向量加法：

```c++
__global__ void vectorAdd(float* A, float* B, float* C, int N) {
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    C[i] = A[i] + B[i];
}
```

这个 kernel 在 RTX 4090 上能达到 920 GB/s（91%峰值带宽），因为：

- Warp 内 32 个线程访问`A[0]到A[31]`（连续）
- Warp 内 32 个线程访问`B[0]到B[31]`（连续）
- Warp 内 32 个线程写入`C[0]到C[31]`（连续）

**关键**：索引计算`i = threadIdx.x + blockIdx.x * blockDim.x`保证了 Warp 内线程访问的地址是连续的。

但如果算法本身要求非连续访问（如矩阵转置、图遍历），就无法避免性能损失。

这时需要用 Shared Memory 做为中转站。

---

## 3. Shared Memory —— 片上高速缓存

### 3.1 它是什么

Shared Memory 是位于 SM 芯片上的高速 SRAM，专门用于 Block 内的线程共享数据，比 Global 快 20 倍。

```
用办公楼的快递柜类比：

- Global Memory = 市区的物流中心（远但容量大）
- Shared Memory = 办公楼 1 层的快递柜（近但容量小）

如果你今天要收 10 个包裹：
├─ 每次都跑市区取 → 耗时 10 趟
└─ 一次性搬到楼下快递柜 → 之后随时取，耗时 1 趟
```

Shared Memory 就是这个"快递柜"，让 Block 内的线程可以快速共享数据。

和 Global Memory 的对比：

| 特性     | Shared Memory           | Global Memory |
| -------- | ----------------------- | ------------- |
| 延迟     | ~30 周期                | 400-600 周期  |
| 带宽     | ~10 TB/s                | 1 TB/s        |
| 容量     | 48-128 KB/SM            | 24 GB         |
| 作用域   | 同一 Block 内的线程可见 | 所有线程可见  |
| 生命周期 | Kernel 执行期间         | 持久化        |

Shared Memory 快在哪里？

- 物理上在 SM 内部，不需要走 HBM 总线
- 带宽是 Global 的 10 倍
- 延迟是 Global 的 1/20

但它也有限制：

- 容量小（每个 SM 只有 48-128 KB）
- 只在 Block 内可见，无法跨 Block 共享
- Kernel 结束后数据消失

### 3.2 Shared Memory 的标准使用流程

使用 Shared Memory 的典型模式是**三步走**：

```c++
__shared__ float cache[BLOCK_SIZE];  // 声明Shared Memory

// Step 1: 从Global加载到Shared（合并访问）
int tid = threadIdx.x;
int globalIdx = tid + blockIdx.x * blockDim.x;
cache[tid] = input[globalIdx];
__syncthreads();  // 等待所有线程加载完成

// Step 2: 在Shared上多次读写（快！）
float sum = 0;
for (int i = 0; i < BLOCK_SIZE; i++) {
    sum += cache[i] * weight[i];  // 反复访问cache
}

// Step 3: 写回Global（合并访问）
output[globalIdx] = sum;
```

关键点：

1. **`__syncthreads()`**：确保所有线程都完成加载后，才能开始使用 Shared Memory
2. **多次访问**：如果数据只用 1 次，不值得搬到 Shared
3. **合并访问 Global**：Step 1 和 Step 3 都要保证合并访问

### 3.3 何时应该使用 Shared Memory？

不是所有场景都适合 Shared Memory。判断标准：

| 场景                             | 是否适合 | 原因                     |
| -------------------------------- | -------- | ------------------------ |
| 数据被多次访问（卷积、矩阵乘法） | ✅       | 一次搬运，多次使用       |
| 线程间需要交换数据（Reduction）  | ✅       | 必须通过 Shared 通信     |
| 避免非合并访问（矩阵转置）       | ✅       | 用 Shared 做中转         |
| 数据只访问 1 次（向量加法）      | ❌       | 搬运的开销 > 收益        |
| 需要跨 Block 共享数据            | ❌       | Shared 只在 Block 内可见 |

简单的判断公式：

```
收益 = (节省的Global访问次数) × (Global延迟)
开销 = (加载到Shared的时间) + (从Shared读取的时间)

如果 收益 > 开销，就用Shared Memory
```

对于访问次数 N：

- N = 1：不用 Shared，直接访问 Global
- N = 2-3：可能持平，视情况而定
- N ≥ 5：应该用 Shared，收益明显

让我们看两个简单的实战案例。

---

## 4. 实战案例

实际代码中如何应用这些原则？

### 4.1 案例 1：向量点积（Block 内归约）

问题：计算两个向量的点积 A·B = Σ(A[i] × B[i])

**普通版本**：每个线程直接累加

```c++
__global__ void dot_naive(float* A, float* B, float* result, int N) {
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    if (i < N) {
        atomicAdd(result, A[i] * B[i]);  // 所有线程竞争同一个地址
    }
}
```

问题：上万个线程同时对 result 做原子加法，造成严重的序列化。

**优化思路**：用 Shared Memory 做两级归约

1. 每个 Block 内部先求和（用 Shared Memory）
2. 每个 Block 只写一次 Global（用 atomic）

```c++
__global__ void dot_shared(float* A, float* B, float* result, int N) {
    __shared__ float cache[256];  // 假设Block大小=256

    int tid = threadIdx.x;
    int i = tid + blockIdx.x * blockDim.x;

    // Step 1: 每个线程计算一个乘积，存入Shared
    cache[tid] = (i < N) ? A[i] * B[i] : 0;
    __syncthreads();

    // Step 2: Block内归约（树形求和）
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            cache[tid] += cache[tid + stride];
        }
        __syncthreads();
    }

    // Step 3: Block 0 代表写回Global
    if (tid == 0) {
        atomicAdd(result, cache[0]);
    }
}
```

工作原理（以 8 个线程为例）：

```
初始状态：cache = [1, 2, 3, 4, 5, 6, 7, 8]

第1轮（stride=4）：
  Thread 0: cache[0] += cache[4]  → cache[0] = 1+5 = 6
  Thread 1: cache[1] += cache[5]  → cache[1] = 2+6 = 8
  Thread 2: cache[2] += cache[6]  → cache[2] = 3+7 = 10
  Thread 3: cache[3] += cache[7]  → cache[3] = 4+8 = 12
  结果：cache = [6, 8, 10, 12, 5, 6, 7, 8]

第2轮（stride=2）：
  Thread 0: cache[0] += cache[2]  → cache[0] = 6+10 = 16
  Thread 1: cache[1] += cache[3]  → cache[1] = 8+12 = 20
  结果：cache = [16, 20, 10, 12, 5, 6, 7, 8]

第3轮（stride=1）：
  Thread 0: cache[0] += cache[1]  → cache[0] = 16+20 = 36
  结果：cache = [36, 20, 10, 12, 5, 6, 7, 8]

最终cache[0] = 36 = 1+2+3+4+5+6+7+8 ✅
```

性能对比（1M 个元素）：

| 版本        | 带宽/算力 | 耗时    | 加速比    |
| ----------- | --------- | ------- | --------- |
| 普通版本    | 15 GB/s   | 3.2 ms  | 1.0×      |
| Shared 版本 | 380 GB/s  | 0.13 ms | **24.6×** |

为什么快这么多？

- 普通版本：100 万次 atomic 操作，高度串行化
- Shared 版本：只有约 4000 次 atomic 操作（每个 Block 1 次）

### 4.2 案例 2：1D 卷积（数据复用）

问题：对数组做简单的 3 点平均

```
output[i] = (input[i-1] + input[i] + input[i+1]) / 3
```

**直接实现**：

```c++
__global__ void smooth_naive(float* input, float* output, int N) {
    int i = threadIdx.x + blockIdx.x * blockDim.x + 1;
    if (i < N - 1) {
        output[i] = (input[i-1] + input[i] + input[i+1]) / 3.0f;
    }
}
```

看起来很简单，但有个隐藏问题：**数据重复读取**。

考虑一个 Block 内 256 个线程：

- Thread 0 读取 input[0], input[1], input[2]
- Thread 1 读取 input[1], input[2], input[3]
- Thread 2 读取 input[2], input[3], input[4]
- ...

每个 input[i]被相邻的 3 个线程读取，总共从 Global 读取了 256×3 = 768 次。

**Shared Memory 优化**：把数据加载到 Shared(用`__shared__`声明)，每个元素只从 Global 读 1 次

```c++
#define BLOCK_SIZE 256

__global__ void smooth_shared(float* input, float* output, int N) {
    __shared__ float tile[BLOCK_SIZE + 2];  // 多加载2个边界元素

    int tid = threadIdx.x;
    int i = tid + blockIdx.x * BLOCK_SIZE + 1;

    // Step 1: 加载到Shared（包括halo区域）
    tile[tid + 1] = input[i];

    // 边界线程负责加载额外的halo元素
    if (tid == 0) {
        tile[0] = input[i - 1];  // 左边界
    }
    if (tid == BLOCK_SIZE - 1) {
        tile[BLOCK_SIZE + 1] = input[i + 1];  // 右边界
    }
    __syncthreads();

    // Step 2: 从Shared计算
    if (i < N - 1) {
        output[i] = (tile[tid] + tile[tid + 1] + tile[tid + 2]) / 3.0f;
    }
}
```

Halo 区域示意图（8 个线程为例）：

```
Block处理input[8:16]，但需要访问input[7:17]

Global Memory:
  ... [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] ...
       ↑   ↑                                   ↑    ↑
       |   |        主体数据                   |    |
      halo                                    halo

Shared Memory (tile):
  [0] [1] [2]  [3]  [4]  [5]  [6]  [7]  [8] [9]
   ↑                                          ↑
  左halo                                    右halo
```

性能对比（1M 个元素）：

| 版本        | 带宽     | Global 读取次数 | 加速比   |
| ----------- | -------- | --------------- | -------- |
| 普通版本    | 280 GB/s | 3M 次           | 1.0×     |
| Shared 版本 | 650 GB/s | 1M 次           | **2.3×** |

为什么快？

- 普通版本：每个元素从 Global 读 3 次
- Shared 版本：每个元素从 Global 读 1 次，从 Shared 读 3 次

Shared 的读取延迟只有 Global 的 1/20，即使多读了几次 Shared，总体仍然更快。

### 4.3 两个案例的共同点

这两个优化都遵循相同的模式：

1. **识别数据复用**：

   - 案例 1：Block 内所有线程都需要访问 cache 数组
   - 案例 2：相邻线程读取重叠的数据

2. **用 Shared 做中转**：

   - 从 Global 合并加载到 Shared（1 次）
   - 从 Shared 多次读取（快）

3. **收益计算**：
   - 案例 1：减少 100 万次 atomic → 4000 次 atomic
   - 案例 2：减少 3M 次 Global 读取 → 1M 次 Global 读取

核心原则：**当数据被多个线程访问 ≥2 次时，考虑 Shared Memory**。

---

## 5. 实用建议

实际开发中如何选择内存策略？

### 5.1 决策流程图

```
开始写Kernel
    ↓
需要线程间通信/同步？
    ├─ 是 → 必须用Shared Memory
    └─ 否 → 继续判断
            ↓
        数据被访问≥3次？
            ├─ 是 → 考虑Shared Memory
            └─ 否 → 继续判断
                    ↓
                能否保证合并访问？
                    ├─ 是 → 直接用Global Memory
                    └─ 否 → 用Shared Memory做中转
```

### 5.2 常见错误及修复

| 错误                  | 表现           | 危害        | 修复方法                   |
| --------------------- | -------------- | ----------- | -------------------------- |
| 非合并访问            | 带宽利用率<30% | 慢 10-32×   | 调整索引，确保 Warp 内连续 |
| 忘记`__syncthreads()` | 结果错误       | 数据竞争    | 每次 Shared 读写后同步     |
| 过度使用 Shared       | Occupancy 下降 | SM 利用率低 | 按需分配，单 Block<48KB    |
| 单次访问用 Shared     | 性能无提升     | 浪费寄存器  | 直接访问 Global            |

### 5.3 性能分析工具

编译时查看资源使用：

```bash
nvcc -Xptxas -v kernel.cu

输出示例：
ptxas info: Used 32 registers, 4096 bytes smem, 0 bytes lmem
                    ↑ 寄存器    ↑ Shared Memory
```

运行时性能分析：

```bash
nsys profile --stats=true ./program

关键指标：
- Memory Throughput: 目标 >80% 峰值
- Achieved Occupancy: 目标 >50%
- Uncoalesced Global Accesses: 目标 <5%
```

Nsight Compute 查看详细信息：

```bash
ncu --set full ./program

重点关注：
- SOL Memory: 内存瓶颈百分比（目标<70%）
- Global Load/Store Throughput: 合并访问效率
- Shared Memory Conflicts: Bank冲突率（目标0%）
```

### 5.4 优化的优先级

不要一开始就优化所有细节，按这个顺序逐步优化：

1. **首先：确保合并访问**（影响最大，10-32×）
2. **其次：使用 Shared Memory**（有数据复用时，2-5×）
3. **然后：调整 Block 大小**（影响 Occupancy，1.2-2×）
4. **最后：消除 Bank Conflict**（影响较小，1.1-1.5×）

每一步都要用 profiler 验证，不要盲目优化。

---

## 6. 总结

### 三张核心表格

**表格 1：内存层次对比**

| 内存类型      | 延迟     | 容量      | 带宽     | 典型用途         |
| ------------- | -------- | --------- | -------- | ---------------- |
| Register      | 1 周期   | 256 KB/SM | 100 TB/s | 临时变量         |
| Shared Memory | 30 周期  | 128 KB/SM | 10 TB/s  | Block 内数据复用 |
| L2 Cache      | 200 周期 | 96 MB     | -        | 自动缓存         |
| Global Memory | 600 周期 | 24 GB     | 1 TB/s   | 主要数据存储     |

**表格 2：优化效果对比**

| 场景                      | 基准版本 | 优化后   | 加速比    | 关键技术     |
| ------------------------- | -------- | -------- | --------- | ------------ |
| 向量加法（非合并 → 合并） | 28 GB/s  | 920 GB/s | **32.8×** | 调整索引     |
| 向量点积（atomic→Shared） | 15 GB/s  | 380 GB/s | **24.6×** | Block 内归约 |
| 1D 卷积（重复读 →Shared） | 280 GB/s | 650 GB/s | **2.3×**  | 数据复用     |

**表格 3：何时使用 Shared Memory**

| 判断条件                        | 是否使用 |
| ------------------------------- | -------- |
| 数据被同一 Block 的多个线程读取 | ✅       |
| 需要线程间通信或同步            | ✅       |
| 避免 Global Memory 非合并访问   | ✅       |
| 数据访问次数 ≥3 次              | ✅       |
| 数据只访问 1-2 次               | ❌       |
| 需要跨 Block 共享数据           | ❌       |

### 五条核心要点

1. **内存带宽是 GPU 性能的真正瓶颈**

2. **合并访问是 Global Memory 的生命线**  
   非合并访问会导致 32 倍性能损失。Warp 内线程应访问连续地址。

3. **Shared Memory 是数据复用的关键工具**  
   当数据被多次访问时，用 Shared Memory 做中转可以加速 2-5 倍。

4. **不是所有场景都需要 Shared Memory**  
   单次访问的数据直接用 Global 更简单。盲目使用 Shared 可能降低 Occupancy。

5. **优化必须用 profiler 验证**  
   理论上的优化不一定实际有效。每次改动都要实测性能，避免过早优化。
